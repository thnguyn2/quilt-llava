{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image tasks with VLM\n",
    "IDEFICS tutorial is based on Flamingo, developed by DeepMind\n",
    "Reference:\n",
    "- [idefics tutorial](https://huggingface.co/docs/transformers/v4.41.2/en/tasks/idefics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes>=0.39.0 transformers==4.41.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_ID = \"HuggingFaceM4/idefics-9b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, IdeficsForVisionText2Text, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # The weights are saved in 4 bit\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # The calculations are done in 16 bit float, https://huggingface.co/docs/transformers/v4.41.2/en/tasks/idefics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:46<00:00,  2.43s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "model = IdeficsForVisionText2Text.from_pretrained(_MODEL_ID, device_map=\"auto\", quantization_config=quantization_config)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captioning - Image2Text task\n",
    "The model receives text and images.\n",
    "To captionize the image, the model only needs a processed input image to generate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = [\n",
    "    \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 32000, 32001, 32000]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:1'), 'pixel_values': tensor([[[[[-0.5660, -0.5806, -0.5660,  ..., -0.7850, -0.7850, -0.7850],\n",
      "           [-0.5660, -0.5660, -0.5514,  ..., -0.7704, -0.7850, -0.7850],\n",
      "           [-0.5514, -0.5368, -0.5514,  ..., -0.7704, -0.7704, -0.7850],\n",
      "           ...,\n",
      "           [-1.1791, -1.1207, -1.1499,  ..., -1.1499, -1.1645, -1.1207],\n",
      "           [-1.1645, -1.1791, -1.2083,  ..., -0.8726, -0.9893, -1.0769],\n",
      "           [-1.2375, -1.2375, -1.2375,  ..., -1.0477, -1.0331, -1.1353]],\n",
      "\n",
      "          [[-0.2213, -0.2063, -0.1913,  ..., -0.7166, -0.7316, -0.7316],\n",
      "           [-0.1463, -0.1313, -0.1163,  ..., -0.7166, -0.7166, -0.7316],\n",
      "           [-0.0862, -0.0712, -0.0562,  ..., -0.7166, -0.7166, -0.7316],\n",
      "           ...,\n",
      "           [-0.8366, -0.7016, -0.7166,  ..., -1.1518, -1.1218, -1.0918],\n",
      "           [-0.8366, -0.8366, -0.8366,  ..., -1.0167, -1.0467, -1.0617],\n",
      "           [-0.9567, -1.0017, -0.9567,  ..., -1.0467, -1.0317, -1.1218]],\n",
      "\n",
      "          [[-0.0440, -0.0298, -0.0156,  ..., -0.5986, -0.5844, -0.5986],\n",
      "           [ 0.0413,  0.0555,  0.0698,  ..., -0.5844, -0.5844, -0.5844],\n",
      "           [ 0.1124,  0.1409,  0.1551,  ..., -0.5844, -0.5986, -0.5844],\n",
      "           ...,\n",
      "           [-0.9114, -0.8688, -0.8830,  ..., -0.8972, -0.8830, -0.8545],\n",
      "           [-0.9114, -0.8830, -0.8972,  ..., -0.8688, -0.8545, -0.8545],\n",
      "           [-0.9114, -0.9114, -0.8972,  ..., -0.8403, -0.8403, -0.8688]]]]],\n",
      "       device='cuda:1'), 'image_attention_mask': tensor([[[0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [1]]], device='cuda:1')}\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "processed_inputs = processor(image_path, return_tensors=\"pt\").to(\"cuda\")\n",
    "# Note that the processed inputs have some token ids + pixel values that stores the image pixel values.\n",
    "print(processed_inputs)\n",
    "print(processed_inputs['input_ids'].shape)\n",
    "print(processed_inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token ids for the bad words\n",
    "bad_words_id = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model generates <image> or <fake_token_around_image> when there is no image fed into the model.\n",
    "generated_output_ids = model.generate(**processed_inputs, max_new_tokens=20, bad_words_ids=bad_words_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A puppy in a flower bed\n"
     ]
    }
   ],
   "source": [
    "decoded_text = processor.batch_decode(generated_output_ids, skip_special_tokens=True)[0]\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompted image captioning\n",
    "Use text and image prompts as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n",
    "    \"This is an image of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 32000, 32001, 32000,   910,   338,   385,  1967,   310]],\n",
      "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1'), 'pixel_values': tensor([[[[[1.4632, 1.4632, 1.4632,  ..., 1.4048, 1.4048, 1.4048],\n",
      "           [1.4632, 1.4632, 1.4632,  ..., 1.4048, 1.4048, 1.4048],\n",
      "           [1.4486, 1.4486, 1.4486,  ..., 1.4194, 1.4194, 1.4194],\n",
      "           ...,\n",
      "           [1.3026, 1.3026, 1.3026,  ..., 1.4340, 1.4340, 1.4340],\n",
      "           [1.2734, 1.2880, 1.2880,  ..., 1.4340, 1.4340, 1.4340],\n",
      "           [1.2734, 1.2880, 1.2880,  ..., 1.4340, 1.4340, 1.4340]],\n",
      "\n",
      "          [[1.5796, 1.5796, 1.5796,  ..., 1.5646, 1.5646, 1.5646],\n",
      "           [1.5796, 1.5796, 1.5796,  ..., 1.5646, 1.5646, 1.5646],\n",
      "           [1.5646, 1.5646, 1.5646,  ..., 1.5796, 1.5796, 1.5796],\n",
      "           ...,\n",
      "           [1.4446, 1.4446, 1.4446,  ..., 1.5496, 1.5496, 1.5496],\n",
      "           [1.4295, 1.4446, 1.4446,  ..., 1.5496, 1.5496, 1.5496],\n",
      "           [1.4145, 1.4295, 1.4295,  ..., 1.5496, 1.5496, 1.5496]],\n",
      "\n",
      "          [[1.8757, 1.8757, 1.8757,  ..., 1.8473, 1.8473, 1.8473],\n",
      "           [1.8757, 1.8757, 1.8757,  ..., 1.8473, 1.8473, 1.8473],\n",
      "           [1.8615, 1.8615, 1.8615,  ..., 1.8615, 1.8615, 1.8615],\n",
      "           ...,\n",
      "           [1.7762, 1.7762, 1.7904,  ..., 1.8473, 1.8473, 1.8473],\n",
      "           [1.7620, 1.7762, 1.7620,  ..., 1.8473, 1.8473, 1.8473],\n",
      "           [1.7904, 1.7904, 1.7904,  ..., 1.8473, 1.8473, 1.8473]]]]],\n",
      "       device='cuda:1'), 'image_attention_mask': tensor([[[0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1]]], device='cuda:1')}\n",
      "Decoded tokens = <s><fake_token_around_image><image><fake_token_around_image> This is an image of\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs)\n",
    "print(f\"Decoded tokens = {processor.decode(inputs['input_ids'][0], skip_special_tokens=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output token ids = tensor([[    1, 32000, 32001, 32000,   910,   338,   385,  1967,   310,   278,\n",
      "          6666,   434,   310, 10895,  1017,   297,  1570,  3088,  4412, 29889,\n",
      "             2]], device='cuda:1')\n",
      "Decoded text = This is an image of the Statue of Liberty in New York City.\n"
     ]
    }
   ],
   "source": [
    "generated_output = model.generate(**inputs, max_new_tokens=100, bad_words_ids=bad_words_id)\n",
    "decoded_output = processor.batch_decode(generated_output, skip_special_tokens=True)[0]\n",
    "print(f\"Generated output token ids = {generated_output}\")\n",
    "print(f\"Decoded text = {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot prompting\n",
    "Do in-context learning by generating results that mimic the format of the given example. Predicts the next token in the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = [\n",
    "    \"User:\",\n",
    "    \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\",\n",
    "    \"Describe this image.\\n Assistant: An image of A puppy in a flower bed. Fun fact: the dog is 1 foot tall.\\n\",\n",
    "    \"User:\",\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n",
    "    \"Describe this image.\\n Assistant: \",\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  4911, 29901, 32000, 32001, 32000, 20355,   915,   445,  1967,\n",
      "         29889,    13,  4007, 22137, 29901,   530,  1967,   310,   319,  2653,\n",
      "         23717,   297,   263, 28149,  6592, 29889, 13811,  2114, 29901,   278,\n",
      "         11203,   338, 29871, 29896,  3661, 15655, 29889,    13,  2659, 29901,\n",
      "         32000, 32001, 32000, 20355,   915,   445,  1967, 29889,    13,  4007,\n",
      "         22137, 29901]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]], device='cuda:1'), 'pixel_values': tensor([[[[[-0.5660, -0.5806, -0.5660,  ..., -0.7850, -0.7850, -0.7850],\n",
      "           [-0.5660, -0.5660, -0.5514,  ..., -0.7704, -0.7850, -0.7850],\n",
      "           [-0.5514, -0.5368, -0.5514,  ..., -0.7704, -0.7704, -0.7850],\n",
      "           ...,\n",
      "           [-1.1791, -1.1207, -1.1499,  ..., -1.1499, -1.1645, -1.1207],\n",
      "           [-1.1645, -1.1791, -1.2083,  ..., -0.8726, -0.9893, -1.0769],\n",
      "           [-1.2375, -1.2375, -1.2375,  ..., -1.0477, -1.0331, -1.1353]],\n",
      "\n",
      "          [[-0.2213, -0.2063, -0.1913,  ..., -0.7166, -0.7316, -0.7316],\n",
      "           [-0.1463, -0.1313, -0.1163,  ..., -0.7166, -0.7166, -0.7316],\n",
      "           [-0.0862, -0.0712, -0.0562,  ..., -0.7166, -0.7166, -0.7316],\n",
      "           ...,\n",
      "           [-0.8366, -0.7016, -0.7166,  ..., -1.1518, -1.1218, -1.0918],\n",
      "           [-0.8366, -0.8366, -0.8366,  ..., -1.0167, -1.0467, -1.0617],\n",
      "           [-0.9567, -1.0017, -0.9567,  ..., -1.0467, -1.0317, -1.1218]],\n",
      "\n",
      "          [[-0.0440, -0.0298, -0.0156,  ..., -0.5986, -0.5844, -0.5986],\n",
      "           [ 0.0413,  0.0555,  0.0698,  ..., -0.5844, -0.5844, -0.5844],\n",
      "           [ 0.1124,  0.1409,  0.1551,  ..., -0.5844, -0.5986, -0.5844],\n",
      "           ...,\n",
      "           [-0.9114, -0.8688, -0.8830,  ..., -0.8972, -0.8830, -0.8545],\n",
      "           [-0.9114, -0.8830, -0.8972,  ..., -0.8688, -0.8545, -0.8545],\n",
      "           [-0.9114, -0.9114, -0.8972,  ..., -0.8403, -0.8403, -0.8688]]],\n",
      "\n",
      "\n",
      "         [[[ 1.4632,  1.4632,  1.4632,  ...,  1.4048,  1.4048,  1.4048],\n",
      "           [ 1.4632,  1.4632,  1.4632,  ...,  1.4048,  1.4048,  1.4048],\n",
      "           [ 1.4486,  1.4486,  1.4486,  ...,  1.4194,  1.4194,  1.4194],\n",
      "           ...,\n",
      "           [ 1.3026,  1.3026,  1.3026,  ...,  1.4340,  1.4340,  1.4340],\n",
      "           [ 1.2734,  1.2880,  1.2880,  ...,  1.4340,  1.4340,  1.4340],\n",
      "           [ 1.2734,  1.2880,  1.2880,  ...,  1.4340,  1.4340,  1.4340]],\n",
      "\n",
      "          [[ 1.5796,  1.5796,  1.5796,  ...,  1.5646,  1.5646,  1.5646],\n",
      "           [ 1.5796,  1.5796,  1.5796,  ...,  1.5646,  1.5646,  1.5646],\n",
      "           [ 1.5646,  1.5646,  1.5646,  ...,  1.5796,  1.5796,  1.5796],\n",
      "           ...,\n",
      "           [ 1.4446,  1.4446,  1.4446,  ...,  1.5496,  1.5496,  1.5496],\n",
      "           [ 1.4295,  1.4446,  1.4446,  ...,  1.5496,  1.5496,  1.5496],\n",
      "           [ 1.4145,  1.4295,  1.4295,  ...,  1.5496,  1.5496,  1.5496]],\n",
      "\n",
      "          [[ 1.8757,  1.8757,  1.8757,  ...,  1.8473,  1.8473,  1.8473],\n",
      "           [ 1.8757,  1.8757,  1.8757,  ...,  1.8473,  1.8473,  1.8473],\n",
      "           [ 1.8615,  1.8615,  1.8615,  ...,  1.8615,  1.8615,  1.8615],\n",
      "           ...,\n",
      "           [ 1.7762,  1.7762,  1.7904,  ...,  1.8473,  1.8473,  1.8473],\n",
      "           [ 1.7620,  1.7762,  1.7620,  ...,  1.8473,  1.8473,  1.8473],\n",
      "           [ 1.7904,  1.7904,  1.7904,  ...,  1.8473,  1.8473,  1.8473]]]]],\n",
      "       device='cuda:1'), 'image_attention_mask': tensor([[[0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1],\n",
      "         [0, 1]]], device='cuda:1')}\n",
      "<s> User:<fake_token_around_image><image><fake_token_around_image> Describe this image.\n",
      " Assistant: An image of A puppy in a flower bed. Fun fact: the dog is 1 foot tall.\n",
      "User:<fake_token_around_image><image><fake_token_around_image> Describe this image.\n",
      " Assistant:\n",
      "input_ids.shape = torch.Size([1, 52])\n",
      "pixel_values.shape = torch.Size([1, 2, 3, 224, 224])\n",
      "image_attention_mask.shape = torch.Size([1, 52, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(few_shot_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs)\n",
    "print(processor.decode(inputs['input_ids'][0], skip_special_tokens=False))\n",
    "print(f\"input_ids.shape = {inputs['input_ids'].shape}\")  # [B, Numshots]\n",
    "print(f\"pixel_values.shape = {inputs['pixel_values'].shape}\")  # [B, Numshots, C, H, W]\n",
    "print(f\"image_attention_mask.shape = {inputs['image_attention_mask'].shape}\")  # [B, S, Numshots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Describe this image.\n",
      " Assistant: An image of A puppy in a flower bed. Fun fact: the dog is 1 foot tall.\n",
      "User: Describe this image.\n",
      " Assistant: An image of The Statue of Liberty. Fun fact: the statue is 111 feet tall.\n",
      "User:\n",
      "\n",
      "Describe\n"
     ]
    }
   ],
   "source": [
    "print(processor.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA\n",
    "Open-ended questions based on an image. Slightly different than image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Instruction: Provide an answer to the question. Use the image to answer.\n",
      "<fake_token_around_image><image><fake_token_around_image> Question: Where are these people and what is the girl's name? Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"Instruction: Provide an answer to the question. Use the image to answer.\\n\",\n",
    "    \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n",
    "    \"Question: Where are these people and what is the girl's name? Answer:\",\n",
    "]\n",
    "inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(processor.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Provide an answer to the question. Use the image to answer.\n",
      " Question: Where are these people and what is the girl's name? Answer: They are in a park in New York City. The girl's name is Lily.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_id, do_sample=False)\n",
    "print(processor.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CATEGORIES = [\n",
    "    'animals','vegetables', 'city landscape', 'cars', 'office'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    f\"Instruction: Classify the image into one of the following categories: {_CATEGORIES}.\\n\",\n",
    "    \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n",
    "    \"Category: \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  2799,  4080, 29901,  4134,  1598,   278,  1967,   964,   697,\n",
      "           310,   278,  1494, 13997, 29901,  6024, 11576,  1338,   742,   525,\n",
      "           345,   657,  1849,   742,   525, 12690, 24400,   742,   525, 29883,\n",
      "          1503,   742,   525, 20205, 13359,    13, 32000, 32001, 32000, 17943,\n",
      "         29901]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1'), 'pixel_values': tensor([[[[[ 0.4559,  0.3391,  0.2515,  ...,  0.2953,  0.2223, -0.5806],\n",
      "           [ 0.4267,  0.2953,  0.2223,  ...,  0.3829,  0.1493, -0.7850],\n",
      "           [ 0.3391,  0.1931,  0.0325,  ...,  0.3975,  0.0617, -0.9893],\n",
      "           ...,\n",
      "           [-0.5076, -0.4930, -0.5222,  ..., -0.8142, -0.7850, -0.7996],\n",
      "           [-0.5368, -0.4930, -0.4784,  ..., -0.7996, -0.7704, -0.7558],\n",
      "           [-0.4346, -0.3908, -0.4200,  ..., -0.7996, -0.7850, -0.7996]],\n",
      "\n",
      "          [[ 0.0338, -0.1463, -0.2663,  ..., -0.2063, -0.2513, -0.8516],\n",
      "           [-0.0562, -0.2363, -0.3864,  ..., -0.1313, -0.3264, -0.9867],\n",
      "           [-0.2213, -0.4014, -0.5665,  ..., -0.0862, -0.3864, -1.0918],\n",
      "           ...,\n",
      "           [-1.0467, -1.0317, -1.0467,  ..., -1.1818, -1.1668, -1.1818],\n",
      "           [-1.0767, -1.0317, -1.0467,  ..., -1.1818, -1.1518, -1.1368],\n",
      "           [-0.9867, -0.9717, -1.0017,  ..., -1.1818, -1.1668, -1.1818]],\n",
      "\n",
      "          [[-1.0963, -1.2243, -1.2669,  ..., -0.9825, -1.0110, -1.1674],\n",
      "           [-1.1532, -1.2385, -1.3096,  ..., -0.9541, -1.0252, -1.2100],\n",
      "           [-1.2243, -1.2954, -1.3522,  ..., -0.9256, -1.0394, -1.2100],\n",
      "           ...,\n",
      "           [-1.2527, -1.2527, -1.2811,  ..., -1.3096, -1.2811, -1.2811],\n",
      "           [-1.2811, -1.2527, -1.2811,  ..., -1.2954, -1.2811, -1.2527],\n",
      "           [-1.2669, -1.2527, -1.2669,  ..., -1.3096, -1.2954, -1.2954]]]]],\n",
      "       device='cuda:1'), 'image_attention_mask': tensor([[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [0],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1]]], device='cuda:1')}\n",
      "<s> Instruction: Classify the image into one of the following categories: ['animals', 'vegetables', 'city landscape', 'cars', 'office'].\n",
      "<fake_token_around_image><image><fake_token_around_image> Category:\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs)\n",
    "print(processor.decode(inputs['input_ids'][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output = Instruction: Classify the image into one of the following categories: ['animals', 'vegetables', 'city landscape', 'cars', 'office'].\n",
      " Category: Vegetables\n"
     ]
    }
   ],
   "source": [
    "bad_words_id = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=3, bad_words_ids=bad_words_id)\n",
    "print(f\"Model output = {processor.batch_decode(generated_ids, skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-guided text generation\n",
    "Write a story about the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    \"Instruction: Use the image to write a story.\\n\",\n",
    "    \"https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80\",\n",
    "    \"Story: \\n\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Instruction: Use the image to write a story.\n",
      "<fake_token_around_image><image><fake_token_around_image> Story: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(processor.decode(inputs['input_ids'][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=200, bad_words_ids=bad_words_id, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geneated story = Instruction: Use the image to write a story.\n",
      " Story: \n",
      "I was walking down the street when I saw a house with a red door.  I thought to myself, “I wonder what’s on the other side of that red door?”  So, I knocked on the door.  The door opened, and there stood a beautiful woman.  She was wearing a red dress, and she had red lipstick on her lips.  She said to me, “Come in, come in.  I’ve been waiting for you.”\n",
      "\n",
      "I walked into the house, and there was a red couch.  I sat down on the couch, and the woman sat down next to me.  She said to me, “I’ve been waiting for you, too.”\n",
      "\n",
      "I said to her, “What do you mean, you’ve been waiting for me?”\n",
      "\n",
      "She said to me, “I’ve been waiting for you for a long time.”\n",
      "\n",
      "I said to her,\n"
     ]
    }
   ],
   "source": [
    "print(f\"Geneated story = {processor.batch_decode(generated_ids, skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:48<00:00,  2.56s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "_FINED_TUNED_ID = \"HuggingFaceM4/idefics-9b-instruct\"\n",
    "model = IdeficsForVisionText2Text.from_pretrained(_MODEL_ID, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(_FINED_TUNED_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"User: What is in this image?\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n",
    "    \"<end_of_utterance>\",  # This is a special token that tells the model that the user has finished speaking.\n",
    "    \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n",
    "    \"\\nUser:\",\n",
    "    \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n",
    "    \"And who is that?<end_of_utterance>\",\n",
    "    \"\\nAssistant:\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values.shape = torch.Size([1, 2, 3, 224, 224])\n",
      "<s> User: What is in this image?<fake_token_around_image><image><fake_token_around_image><end_of_utterance> \n",
      "Assistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance> \n",
      "User:<fake_token_around_image><image><fake_token_around_image> And who is that?<end_of_utterance> \n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(\"cuda\")   \n",
    "print(f\"pixel_values.shape = {inputs['pixel_values'].shape}\")\n",
    "print(processor.decode(inputs['input_ids'][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_token = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# The call belows give an error!\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100, bad_words_ids=bad_words_id, eos_token_id=exit_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
