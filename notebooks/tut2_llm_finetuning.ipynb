{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset prepatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 650000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.33.3 in /opt/conda/envs/qllava/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: datasets==3.0.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: evaluate in /opt/conda/envs/qllava/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: peft==0.5.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from transformers==4.33.3) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/qllava/lib/python3.10/site-packages (from datasets==3.0.1) (3.10.10)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/qllava/lib/python3.10/site-packages (from peft==0.5.0) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from peft==0.5.0) (2.0.1+cu117)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/qllava/lib/python3.10/site-packages (from peft==0.5.0) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (1.15.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from aiohttp->datasets==3.0.1) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.3) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from requests->transformers==4.33.3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from requests->transformers==4.33.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from requests->transformers==4.33.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from requests->transformers==4.33.3) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/qllava/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/qllava/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1.4)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/qllava/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0) (3.30.4)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/qllava/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0) (18.1.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from pandas->datasets==3.0.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/qllava/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.5.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# PEFT training does not exist with 4.33.2\n",
    "!pip install transformers==4.33.3 datasets==3.0.1 evaluate peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       " \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:2]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_MODEL_ID = \"bert-base-cased\"  # Find a list of model id at https://huggingface.co/models\n",
    "# More details about this model can be found at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_sample, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])  # The model does not accept text input, we can't copy a list to GPUS\n",
    "tokenized_dataset.set_format(\"torch\")  # Convert list to torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([650000, 512])\n",
      "torch.Size([1000, 512])\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "print(tokenized_dataset['train']['input_ids'].shape)\n",
    "print(small_train_dataset['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-20 20:01:47,433] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seq_class_model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=_MODEL_ID, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More details about the training arguments can be found at https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"seq_class_res\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metric = evaluate.load(path=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_acc_metric(eval_pred: Tuple[np.ndarray]):\n",
    "    \"\"\"Computes the accuracy metric.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: A tuple of predicted logits shape (B, N_clas), and labels shape (B,)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    return acc_metric.compute(predictions=np.argmax(logits, axis=-1), references=labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with the Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=seq_class_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=_compute_acc_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(seq_class_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps = 625\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "print(f\"num_training_steps = {num_training_steps}\")\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = seq_class_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = seq_class_model.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "prog_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        device_batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = seq_class_model(**device_batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        prog_bar.update(1)\n",
    "    print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the inference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "seq_class_model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    device_batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = seq_class_model(**device_batch)\n",
    "    acc_metric.add_batch(\n",
    "        predictions=torch.argmax(outputs.logits, dim=-1),\n",
    "        references=device_batch[\"labels\"],\n",
    "    )\n",
    "print(acc_metric.compute())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a PEFT adapter and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seq_class_model_2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=_MODEL_ID, \n",
    "    num_labels=5,\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_class_model_2.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer = Trainer(\n",
    "    model=seq_class_model_2,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=_compute_acc_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/jupyter-users-home/tan-2enguyen/quilt-llava/wandb/run-20241020_200319-r007mzok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://pathai.wandb.io/machine-learning/huggingface/runs/r007mzok' target=\"_blank\">vibrant-dust-446</a></strong> to <a href='https://pathai.wandb.io/machine-learning/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://pathai.wandb.io/machine-learning/huggingface' target=\"_blank\">https://pathai.wandb.io/machine-learning/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://pathai.wandb.io/machine-learning/huggingface/runs/r007mzok' target=\"_blank\">https://pathai.wandb.io/machine-learning/huggingface/runs/r007mzok</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.633789</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.632812</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.631836</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=1.6510209986772486, metrics={'train_runtime': 82.0843, 'train_samples_per_second': 36.548, 'train_steps_per_second': 2.303, 'total_flos': 811097699328000.0, 'train_loss': 1.6510209986772486, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
