{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning notebook for the Llava using the Quilt-Pretrained dataset\n",
    "This code will use the existing code repo as much as possible.\n",
    "Its goal is for quickly prototyping the Llava-based chat system.\n",
    "Reference\n",
    "- [Llava doc](https://huggingface.co/docs/transformers/en/model_doc/llava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-28 19:18:08,877] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Union\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llava.conversation import conv_llava_plain\n",
    "from llava.train.train import ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VISION_TOWER = \"wisdomik/QuiltNet-B-32\"  # model_args.vision_tower\n",
    "_MODEL_ID = \"lmsys/vicuna-7b-v1.5\" # model_args.model_name_or_path - Before pretraining\n",
    "#_MODEL_ID = \"wisdomik/Quilt-Llava-v1.5-7b\" # Pretrained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lmsys/vicuna-7b-v1.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arguments = ModelArguments(\n",
    "    model_name_or_path=_MODEL_ID, \n",
    "    version='plain', \n",
    "    freeze_backbone=False, \n",
    "    tune_mm_mlp_adapter=True, \n",
    "    vision_tower=_VISION_TOWER, \n",
    "    mm_vision_select_layer=-2, \n",
    "    pretrain_mm_mlp_adapter=None, \n",
    "    mm_projector_type='mlp2x_gelu', \n",
    "    mm_use_im_start_end=False, \n",
    "    mm_use_im_patch_token=False, \n",
    "    mm_vision_select_feature='patch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llama to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.71s/it]\n",
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/qllava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# More details about this model can be found at https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=_MODEL_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _MODEL_ID,\n",
    "    model_max_length=2048,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation(system='', roles=('', ''), messages=(), offset=0, sep_style=<SeparatorStyle.PLAIN: 4>, sep='\\n', sep2=None, version='Unknown', skip_next=False)\n"
     ]
    }
   ],
   "source": [
    "conversation_lib = conv_llava_plain  # model_args.version == 'plain'\n",
    "print(conv_llava_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_model().initialize_vision_modules(\n",
    "    model_args=model_arguments,\n",
    "    fsdp=None,  # Don't use FSD https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html\n",
    ")\n",
    "model = model.to(device=device, dtype=torch.float16)\n",
    "llamma_model = model.get_model()\n",
    "lm_head = model.lm_head\n",
    "vision_tower = llamma_model.get_vision_tower()\n",
    "mm_projector = llamma_model.mm_projector\n",
    "token_embedder = llamma_model.embed_tokens\n",
    "image_processor = vision_tower.image_processor   # cropsize of (224, 224) for the \"lmsys/vicuna-7b-v1.5\" model IDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the mm projector only. Disable the gradient on all other blocks\n",
    "model.requires_grad_(False)\n",
    "for p in model.get_model().mm_projector.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize_vision_tokenizer(model_args=model_arguments, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_IMAGE_FOLDER = \"/jupyter-users-home/tan-2enguyen/datasets/pathology/quilt/quilt_llava/Quilt-LLaVA-Pretrain/quilt_1m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from transformers import AutoProcessor\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quilt_llava_dataset = load_dataset(\"wisdomik/Quilt-LLaVA-Pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset = Dataset({\n",
      "    features: ['image', 'conversations', 'id'],\n",
      "    num_rows: 650995\n",
      "})\n",
      "test dataset = Dataset({\n",
      "    features: ['image', 'conversations', 'id'],\n",
      "    num_rows: 72333\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds_with_split = quilt_llava_dataset['train'].train_test_split(test_size=0.1)\n",
    "train_ds = ds_with_split['train']\n",
    "test_ds = ds_with_split['test']\n",
    "print(f\"train dataset = {train_ds}\")\n",
    "print(f\"test dataset = {test_ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TOKEN_INDEX = -200\n",
    "IGNORE_INDEX = -100 \n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from llava.conversation import conv_llava_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenizize_prompt(prompt: str, tokenizer: AutoTokenizer, return_tensor: str) -> Union[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"Tokenize the prompt.\n",
    "    \n",
    "    This function will do the following:\n",
    "        - Separate the prompt into multiple parts, tokenize them separately.\n",
    "        - For each chunk, get rid of the BOS token if it is the first token, insert the IMAGE_TOKEN_INDEX in between the chunks\n",
    "        - Convert to tensor from list of token ids if requested.\n",
    "    Args:\n",
    "        prompt: The string for the input prompt\n",
    "        tokenizer: The tokenizer to tokenize the prompt.\n",
    "        return_tensor (optional): If True, return the tensor of token ids. Otherwise, return the list of token ids. Defaults to True.\n",
    "        \n",
    "    Reference:\n",
    "        tokenizer_image_token - https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/mm_utils.py#L43\n",
    "    \"\"\"\n",
    "    tokenized_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(DEFAULT_IMAGE_TOKEN)]\n",
    "    input_ids = [tokenizer.bos_token_id]  # Add the BOS token\n",
    "    \n",
    "    for chunk in tokenized_chunks[:-1]:\n",
    "        if chunk[0] == tokenizer.bos_token_id:\n",
    "            input_ids.extend(chunk[1:])\n",
    "        else:\n",
    "            input_ids.extend(chunk)\n",
    "        input_ids.append(IMAGE_TOKEN_INDEX)\n",
    "    \n",
    "    # Add the last part of the prompt\n",
    "    if tokenized_chunks[-1][0] == tokenizer.bos_token_id:\n",
    "        input_ids.extend(tokenized_chunks[-1][1:])\n",
    "    else:\n",
    "        input_ids.extend(tokenized_chunks[-1])\n",
    "        \n",
    "    if return_tensor == \"pt\":\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"A dataset for caption generation from a chat dataset.\n",
    "    \n",
    "    Args:\n",
    "        image_folder: The path to the image folder\n",
    "        tokenizer: The tokenizer to tokenize the caption.\n",
    "        dataset: The dataset containing the image file name and the caption from the chat dataset\n",
    "        sequence_length: The length of the sequence to pad.\n",
    "    \n",
    "    References:\n",
    "        llava.train.train import LazySupervisedDataset\n",
    "    \"\"\"\n",
    "    def __init__(self, image_folder: str, tokenizer: AutoTokenizer, image_processor, dataset: Dataset, max_sequence_length: int = 256) -> None:\n",
    "        self._image_folder = Path(image_folder)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._dataset = dataset\n",
    "        self._image_processor = image_processor\n",
    "        self._max_sequence_length = max_sequence_length \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: Union[int, List[int]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Get one text samples from the conversation.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        Reference:\n",
    "            LazySupervisedDataset\n",
    "        \"\"\"    \n",
    "        if isinstance(idx, int):\n",
    "            idx = [idx]\n",
    "            \n",
    "        raw_samples = self._dataset[idx]\n",
    "        images = torch.tensor([np.array(Image.open(str(Path(self._image_folder) / file_name)).convert('RGB')) for file_name in raw_samples['image']])\n",
    "        \n",
    "        formated_conversation = self._generate_prompt_from_conversation(raw_samples['conversations'])\n",
    "        raw_input_ids = self._tokenize_prompt_to_input_ids(formated_conversation)\n",
    "        \n",
    "        padded_input_ids = pad_sequence(\n",
    "            raw_input_ids, \n",
    "            batch_first=True, \n",
    "            padding_value=self._tokenizer.pad_token_id\n",
    "        )  # Reference DataCollatorForSupervisedDataset.__call__()\n",
    "        \n",
    "        padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            self._compute_target_labels(input_ids=raw_input_ids, prompts=formated_conversation),\n",
    "            batch_first=True,\n",
    "            padding_value=IGNORE_INDEX\n",
    "        )  #https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/train/train.py#L725\n",
    "        \n",
    "        return {\n",
    "            'image': self._image_processor.preprocess(images,  return_tensors='pt')['pixel_values'].type(torch.float16),\n",
    "            'input_ids': padded_input_ids.type(torch.int64),\n",
    "            'target': padded_labels.type(torch.int64),\n",
    "            'description': [conv[1]['value'] for conv in raw_samples['conversations']],\n",
    "            'attention_mask': padded_input_ids.ne(self._tokenizer.pad_token_id),  # Ref: https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/train/train.py#L733\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_prompt_from_conversation(conversations: List[List[Dict[str, str]]]) -> List[str]:\n",
    "        \"\"\"Generate the prompt from the conversation.\n",
    "        \n",
    "        Args:\n",
    "            conversations: A list of dictionarly dictionary containing the conversation between the user and the bot. \n",
    "            One sample is for 1 minibatch sample\n",
    "        \n",
    "        Returns:\n",
    "            A string containing the prompt with the image token following the format at\n",
    "            https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/train/train.py#L568\n",
    "        \"\"\"\n",
    "        return [DEFAULT_IMAGE_TOKEN + conversation[1]['value'] + conv_llava_plain.sep for conversation in conversations]\n",
    "    \n",
    "    def _tokenize_prompt_to_input_ids(self, prompts: List[str]) -> List[torch.Tensor]:\n",
    "        \"\"\"Tokenizes the prompts.\n",
    "        \n",
    "        Returns:\n",
    "            A list of tensors containing the tokenized ids of all prompts in the training minibatch. Each prompt has \n",
    "            the format of <bos><IMAGE_TOKEN_INDEX><caption ids><\\n> where IMAGE_TOKEN_INDEX = -200 is the image token index\n",
    "            \n",
    "        Reference:\n",
    "            https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/mm_utils.py#L43\n",
    "        \"\"\"\n",
    "        return [_tokenizize_prompt(prompt=prompt, tokenizer=self._tokenizer, return_tensor='pt') for prompt in prompts]\n",
    "                \n",
    "    def _compute_target_labels(self, input_ids: List[torch.Tensor], prompts: List[str]) -> List[torch.Tensor]:\n",
    "        target_ids = []\n",
    "        for input_id in input_ids:\n",
    "            target_id = input_id.clone()\n",
    "            target_id[:2] = IGNORE_INDEX\n",
    "            target_ids.append(target_id)\n",
    "        return target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_train_ds = CaptionDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    image_folder=_IMAGE_FOLDER,\n",
    "    dataset=train_ds,\n",
    "    image_processor=image_processor,\n",
    ")\n",
    "\n",
    "val_caption_ds = CaptionDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    image_folder=_IMAGE_FOLDER,\n",
    "    dataset=test_ds,\n",
    "    image_processor=image_processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length = 650995\n",
      "dict_keys(['image', 'input_ids', 'target', 'description', 'attention_mask'])\n",
      "tensor([[ -100,  -100,   450,   282,   682,   470, 16749, 10161,   526, 26718,\n",
      "           310,  4457,   375,  9825, 29875,   542,  3637, 19263,   411, 20364,\n",
      "           301,   962,   561,  4858,   459,   493, 29891, 29889,    13]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length = {len(caption_train_ds)}\")\n",
    "print(caption_train_ds[10].keys())\n",
    "print(caption_train_ds[10]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mcaption_train_ds\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'caption_train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(caption_train_ds, batch_size=3, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836/3698654861.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  images = torch.tensor([np.array(Image.open(str(Path(self._image_folder) / file_name)).convert('RGB')) for file_name in raw_samples['image']])\n",
      "/tmp/ipykernel_836/3698654861.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  images = torch.tensor([np.array(Image.open(str(Path(self._image_folder) / file_name)).convert('RGB')) for file_name in raw_samples['image']])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['image'].shape)\n",
    "print(batch['target'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs for the VLM pretraining. LlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal() at https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/model/llava_arch.py#L99\n",
    "# Get image projection LlavaMetaForCausalLM.encode_images()\n",
    "def _prepare_input_labels_for_multimodal(\n",
    "    input_ids: torch.Tensor,\n",
    "    image: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare the input and labels for the multimodal model.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: The input ids of the text and image tokens. A tensor of shape [B, S] that stores the tokenized ids of the input prompt.\n",
    "            Each prompt has the format of <bos><IMAGE_TOKEN_INDEX><caption ids><pad_token>...<pad_token> where \n",
    "            IMAGE_TOKEN_INDEX = -200 is the token to be replaced by the image.\n",
    "            S = 1 + 1 + L + P where L is the length of the caption and P is the padding length. The first 2 ones are for the BOS and IMAGE_TOKEN_INDEX.\n",
    "        image: An image tensor of shape [B, C, H, W] where H, W are the height and width of the image outpt of the image processor.    \n",
    "        labels: The target labels tokenized vector of shape. A tensor of shape [B, S] that stores the tokenized ids of the input prompt.\n",
    "            Each prompt has the format of <ignore token><ignore token><caption ids><ignore token>...<ignore token> where \n",
    "            ignore token = -100 is the ignore token.\n",
    "        attention_mask: The attention mask of the input_ids of shape of shape [B, S] that stores the attention mask of the input prompt.\n",
    "            Each item has a form of [TRUE][TRUE][...all TRUEs for captions ...][...all FALSES for padding...].\n",
    "        \n",
    "    Returns:\n",
    "        An attention mask of shape [B, Sout] where Sout is the length of the output mask. Sout = 1 + Npatch + L + P = (Npatch - 1) + S. Each row has the form of\n",
    "            [TRUE][...TRUE... for image features][...all TRUEs for captions ...][...all FALSES for padding...].\n",
    "        Inputs image beddings of shape [B, Sout, D] where D is the dimension of the image embeddings.\n",
    "        Target labels of shape [B, Sout] where each items contains the tokenized ids of the target labels. It has the shape of\n",
    "            <ignore token>[...all ignore_tokens for image...]<caption ids><ignore token>...<ignore token>\n",
    "            \n",
    "    \"\"\"\n",
    "    image_features = mm_projector(vision_tower(image))   # encode images\n",
    "    batch_size, num_image_feature_token =image_features.shape[:2]\n",
    "    all_samples_input_embeds = []\n",
    "    all_samples_labels = []\n",
    "    for sample_idx, input_id in enumerate(input_ids):\n",
    "        image_token_start = torch.where(input_id == IMAGE_TOKEN_INDEX)[0].item()\n",
    "        cur_new_input_embeds = []\n",
    "        cur_new_labels = []\n",
    "        image_feature = image_features[sample_idx]\n",
    "        cur_new_input_embeds.append(token_embedder(input_id[:image_token_start]))\n",
    "        cur_new_input_embeds.append(image_feature)\n",
    "        cur_input_ids = input_id[image_token_start+1:]\n",
    "        cur_new_input_embeds.append(token_embedder(cur_input_ids))\n",
    "        cur_new_input_embeds = [x.to(image_features.device) for x in cur_new_input_embeds]\n",
    "        cur_new_input_embeds = torch.cat(cur_new_input_embeds, dim=0)\n",
    "        all_samples_input_embeds.append(cur_new_input_embeds)\n",
    "        if labels is not None:\n",
    "            cur_label = labels[sample_idx]\n",
    "            cur_new_labels.append(cur_label[:image_token_start])\n",
    "            cur_new_labels.append(torch.full((num_image_feature_token,), IGNORE_INDEX, device=labels.device, dtype=labels.dtype))\n",
    "            cur_label = cur_label[image_token_start+1:]\n",
    "            cur_new_labels.append(cur_label)\n",
    "            cur_new_labels = torch.cat(cur_new_labels, dim=0)\n",
    "            all_samples_labels.append(cur_new_labels)\n",
    "        \n",
    "    all_samples_input_embeds = torch.stack(all_samples_input_embeds, dim=0)\n",
    "    all_samples_labels = torch.stack(all_samples_labels, dim=0)\n",
    "    \n",
    "    #-1 for the image token.\n",
    "    attention_mask = torch.cat(\n",
    "        (\n",
    "            torch.full((batch_size, num_image_feature_token - 1), True, dtype=attention_mask.dtype, device=attention_mask.device), \n",
    "            attention_mask\n",
    "        ), \n",
    "        dim=1\n",
    "    )\n",
    "    return attention_mask, all_samples_input_embeds, all_samples_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask.shape = torch.Size([3, 88])\n",
      "input_embeds.shape = torch.Size([3, 88, 4096])\n",
      "all_labels.shape = torch.Size([3, 88])\n"
     ]
    }
   ],
   "source": [
    "attention_mask, input_embeds, all_labels = _prepare_input_labels_for_multimodal(\n",
    "    input_ids = batch['input_ids'].to(device=device),\n",
    "    image = batch['image'].to(device=device),\n",
    "    labels = batch['target'].to(device=device),\n",
    "    attention_mask = batch['attention_mask'].to(device=device),\n",
    ")\n",
    "print(f\"attention_mask.shape = {attention_mask.shape}\")\n",
    "print(f\"input_embeds.shape = {input_embeds.shape}\")\n",
    "print(f\"all_labels.shape = {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training forward pass\n",
    "Reference: \n",
    "- [LlavaLlamaForCausalLM.forward()](https://github.com/thnguyn2/quilt-llava/blob/7e70fc39f792ac55de010eb37bff0a6d6f491c13/llava/model/language_model/llava_llama.py#L56)\n",
    "- [LlamaModel.forward()]() - Transformer package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output = llamma_model(\n",
    "    input_ids=None,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=None,\n",
    "    past_key_values=None,\n",
    "    inputs_embeds=input_embeds,\n",
    "    use_cache=None,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    return_dict=True,\n",
    ")\n",
    "output_logits = lm_head(llama_output.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_output.keys() = odict_keys(['last_hidden_state', 'past_key_values'])\n",
      "llama_output.last_hidden_state.shape = torch.Size([3, 88, 4096])\n",
      "lm_out.shape = torch.Size([3, 88, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"llama_output.keys() = {llama_output.keys()}\")\n",
    "print(f\"llama_output.last_hidden_state.shape = {llama_output.last_hidden_state.shape}\")\n",
    "print(f\"lm_out.shape = {output_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([261, 32000])\n",
      "targets.shape = torch.Size([261])\n",
      "loss = 5.765625\n"
     ]
    }
   ],
   "source": [
    "# The predicted logits is delayed by 1 token compared to the target labels i.e. out_logits[0] is the prediction for target[1], out_logits[1] is the prediction for target[2], etc.\n",
    "SHIFT_AMOUNT = 1\n",
    "logits = output_logits[:,:-SHIFT_AMOUNT,:].contiguous()\n",
    "targets = all_labels[:,SHIFT_AMOUNT:].contiguous()\n",
    "logits = logits.view(-1, logits.size(-1))\n",
    "targets = targets.view(-1)\n",
    "loss = CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "print(f\"logits.shape = {logits.shape}\")\n",
    "print(f\"targets.shape = {targets.shape}\")\n",
    "print(f\"loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
