{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        




        // https://github.com/microsoft/DeepSpeed/issues/938#issuecomment-1742254669
        {
            "name": "Pre-training",
            "type": "debugpy",
            "request": "launch",
            "module": "deepspeed.launcher.runner",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "llava/train/train_mem.py",
                "--deepspeed", "./scripts/zero2.json",
                "--model_name_or_path", "lmsys/vicuna-7b-v1.5",
                "--version", "plain",
                "--data_path", "/jupyter-users-home/tan-2enguyen/datasets/pathology/quilt/quilt_llava/Quilt-LLaVA-Pretrain/quilt_pretrain.json",
                "--image_folder", "/jupyter-users-home/tan-2enguyen/datasets/pathology/quilt/quilt_llava/Quilt-LLaVA-Pretrain/quilt_1m",
                "--vision_tower", "wisdomik/QuiltNet-B-32",
                "--mm_projector_type", "mlp2x_gelu",
                "--tune_mm_mlp_adapter", "True",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                //"--bf16", "True",  // Turn this on once we have Ampere GPUs.
                "--fp16", "True",
                "--output_dir", "./checkpoints/quilt-llava-v1.5-7b-pretrain",
                "--num_train_epochs", "1", 
                "--per_device_train_batch_size", "4",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "1",
                "--evaluation_strategy", "no",
                "--save_strategy", "steps",
                "--save_steps", "800",
                "--save_total_limit", "1",
                "--learning_rate", "1e-3",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                "--tf32", "False",  // Set to True if we have Ampere GPUs.
                "--model_max_length", "2048",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "1",
                "--lazy_preprocess", 
                "True",
                "--report_to", "wandb",
            ]
        },
        {
            "name": "VQA Inference",
            "type": "debugpy",
            "module": "llava.serve.cli",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--model-path", 
                "wisdomik/Quilt-Llava-v1.5-7b",
                "--image-file",
                "https://wisdomikezogwo.github.io/images/eval_example_3_.jpg",
                "--load-8bit",
            ]
        },
        {
            "name": "Fine-tuning-VICUNA-LORA",
            "type": "debugpy",
            "request": "launch",
            "module": "deepspeed.launcher.runner",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "llava/train/train_mem.py",
                "--lora_enable", "True",
                "--lora_r", "128",
                "--lora_alpha", "256",
                "--mm_projector_lr", "2e-5",
                "--deepspeed", "./scripts/zero3.json", 
                "--model_name_or_path", "lmsys/vicuna-13b-v1.5",
                "--version", "v1",
                "--data_path", "/jupyter-users-home/tan-2enguyen/datasets/pathology/quilt/quilt_llava/QUILT-LLaVA-Instruct-107K/quilt_instruct_107k.json",
                "--image_folder", "/jupyter-users-home/tan-2enguyen/datasets/pathology/quilt/quilt_llava/quilt_instruct",
                "--vision_tower", "wisdomik/QuiltNet-B-32",
                //"--pretrain_mm_mlp_adapter", "./checkpoints/quilt-llava-v1.5-7b-pretrain/mm_projector.bin",
                "--mm_projector_type", "mlp2x_gelu",
                "--mm_vision_select_layer", "-2",
                "--mm_use_im_start_end", "False",
                "--mm_use_im_patch_token", "False",
                "--image_aspect_ratio", "pad",
                "--group_by_modality_length", "False",
                //"--bf16", "True"
                "--fp16", "True",
                "--output_dir", "./checkpoints/quilt-llava-v1.5-7b-lora-f-1eps",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "12",
                "--per_device_eval_batch_size", "4",
                "--gradient_accumulation_steps", "1",
                "--evaluation_strategy", "no",
                "--save_strategy", "steps",
                "--save_steps", "300",
                "--save_total_limit", "1",
                "--learning_rate", "2e-5",
                "--weight_decay", "0.",
                "--warmup_ratio", "0.03",
                "--lr_scheduler_type", "cosine",
                "--logging_steps", "1",
                //"--tf32", "True",
                "--tf32", "False",
                "--model_max_length", "2048",
                "--gradient_checkpointing", "True",
                "--dataloader_num_workers", "4",
                "--lazy_preprocess", "True",
                "--report_to", "wandb",
            ]
        }
    ]
}



